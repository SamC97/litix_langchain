import os
import csv
import yaml
from PIL import Image
from src.ocr.easyocr import EasyOCR
from src.ocr.tesseract import TesseractOCR
from src.ocr.doctr import DoctrOCR
from src.search import search_title, search_author, search_if_groundwater_mentioned, search_year
from src.utils.image_utils import pdf_to_images
from src.utils.text_utils import crop_markdown_by_page
from src.utils.groundtruth_utils import convert_groundtruth_to_csv
from src.evaluation.metrics import *
from src.visualization.plot_results import plot_detailed_results

class Pipeline:
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        # ------------------------ OCR PART ------------------------
        # Retrieve OCR type from the config and initialize the corresponding OCR engine
        ocr_type = self.config.get('ocr', {}).get('type', 'tesseract').lower()
        self.ocr_type = ocr_type
        if ocr_type == 'tesseract':
            self.ocr_engine = TesseractOCR(lang=self.config['ocr'].get('lang', 'eng'))
        elif ocr_type == 'easyocr':
            self.ocr_engine = EasyOCR(lang_list=self.config['ocr'].get('lang_list', ['en']))
        elif ocr_type == 'doctr':
            self.ocr_engine = DoctrOCR()
        else:
            raise ValueError(f"OCR type {ocr_type} is not supported")
        # -----------------------------------------------------------
    
    
    def prepare_data(self):
        """
        Prepare the data for the pipeline:
        - Convert the ground truth Excel file to CSV.
        """
        if os.path.exists("data/ground_truth.xlsx"):
            if not os.path.exists("data/groundtruth.csv"):
                convert_groundtruth_to_csv()
            else:
                print("Ground truth CSV file already exists - skipping...")
        else:
            print("Ground truth file not found. Please add the ground truth Excel file to data/ground_truth.xlsx.")
    
    
    def run_ocr(self):
        """
        OCR PART:
        Convert PDF files to Markdown files.
        For each PDF, if a corresponding Markdown file does not exist, perform the conversion
        page by page, adding a page break marker at the end of each page.
        """
        print(f"Starting OCR conversion with {self.ocr_type} ...")
        
        pdf_dir = self.config.get('pdf_directory', 'data/pdf')
        base_md_dir = self.config.get('markdown_directory', 'data/markdown')
        # Create a subdirectory specific to the OCR type (e.g. "tesseract" or "easyocr")
        md_dir = os.path.join(base_md_dir, self.ocr_type)
        os.makedirs(md_dir, exist_ok=True)
        
        # List all PDF files in the PDF directory
        pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]
        for pdf_file in pdf_files:
            # Generate the Markdown filename based on the PDF filename
            md_filename = os.path.splitext(pdf_file)[0] + '.md'
            md_filepath = os.path.join(md_dir, md_filename)
            
            if os.path.exists(md_filepath):
                print(f"{md_filename} is already converted and saved in {md_dir} - skipping...")
            else:
                print(f"Converting {pdf_file} to Markdown...")
                pdf_path = os.path.join(pdf_dir, pdf_file)
                images = pdf_to_images(pdf_path)
                full_text = ""
                # Loop through images (pages) and perform OCR on each page
                for idx, img in enumerate(images, start=1):
                    text = self.ocr_engine.ocr(img)
                    # Add a distinctive page break marker to the text
                    page_break_marker = f"\n\n### PAGE_BREAK: {idx} ###\n\n"
                    full_text += text + page_break_marker
                
                # Save the full text as a Markdown file
                with open(md_filepath, 'w', encoding='utf-8') as f:
                    f.write(full_text)
                print(f"{pdf_file} converted and saved as {md_filename} in {md_dir}")


    def run_search(self):
        """
        INFORMATION EXTRACTION PART:
        For each Markdown file generated by the OCR phase, this method:
        - Reads the full content to extract the title (and date) using LLM-based and regex-based methods.
        - Creates a cropped version of the Markdown file (up to a specified page) to extract the author.
        """
        print("Starting information extraction with model {} ...".format(self.config.get('llm', {}).get('model', 'qwen2.5:7b')))
        
        base_md_dir = self.config.get('markdown_directory', 'data/markdown')
        md_dir = os.path.join(base_md_dir, self.ocr_type)
        # List all Markdown files in the directory
        md_files = sorted([f for f in os.listdir(md_dir) if f.lower().endswith('.md')])
        
        # Get the maximum page number for author extraction from config (default to 3 if not provided)
        crop_page = self.config.get('llm', {}).get('crop_page_nb', 3)
        
        # List to store results for CSV export
        results = []
        
        for md_file in md_files:
            md_filepath = os.path.join(md_dir, md_file)
            
            # Read the full Markdown file (for title and date extraction)
            with open(md_filepath, 'r', encoding='utf-8') as f:
                full_text = f.read()
                
            # Create a cropped version of the Markdown file (for author extraction)
            cropped_md_filepath = crop_markdown_by_page(md_filepath, crop_page)
            with open(cropped_md_filepath, 'r', encoding='utf-8') as f:
                cropped_text = f.read()
            
            # Extract title using the full text via an LLM-based method
            print(f"Extracting title from {md_file} ...")
            title = search_title(
                cropped_text,
                model=self.config.get('llm', {}).get('model', 'qwen2.5:7b'),
                max_context_size=self.config.get('llm', {}).get('max_context_size', 2048),
                lang=self.config.get('prompt', {}).get('lang', 'en')
            )
            
            # Extract author using the cropped text via an LLM-based method
            print(f"Extracting author from {md_file} ...")
            author = search_author(
                cropped_text,
                model=self.config.get('llm', {}).get('model', 'qwen2.5:7b'),
                max_context_size=self.config.get('llm', {}).get('max_context_size', 2048),
                lang=self.config.get('prompt', {}).get('lang', 'en')
            )
            
            # Check if the term "groundwater" is mentioned in the full text
            print(f"Checking if groundwater is mentioned in {md_file} ...")
            is_groundwater_mentioned = search_if_groundwater_mentioned(
                full_text,
                model=self.config.get('llm', {}).get('model', 'qwen2.5:7b'),
                max_context_size=self.config.get('llm', {}).get('max_context_size', 2048),
                lang=self.config.get('prompt', {}).get('lang', 'en')
            )
            
            # Extract date using a regex-based method from the full text
            print(f"Extracting date from {md_file} ...")
            date = search_year(full_text)
            
            # Print the extracted information for this Markdown file
            print("\n--------------------------------------------------")
            print(f"Results for {md_file}:")
            print(f"  Title : {title}")
            print(f"  Author: {author}")
            print(f"  Groundwater mentioned: {is_groundwater_mentioned}")
            print(f"  Date  : {date}")
            print("--------------------------------------------------\n")
            
            # Remove the ".md" extension from the filename values in the "Filename" column for consistency
            md_file = md_file.replace(".md", "")
            
            # Save the extracted information for CSV export
            results.append({
                "Filename": md_file,
                "Title": title,
                "Author": author,
                "Publication Year": date,
                "Groundwater level mentioned": is_groundwater_mentioned
            })
            
        # Write the results to a CSV file
        output_csv = self.config.get('output_csv', 'data/extracted_data.csv')
        with open(output_csv, 'w', encoding='utf-8') as csvfile:
            fieldnames = ["Filename", "Title", "Author", "Publication Year", "Groundwater level mentioned"]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for row in results:
                writer.writerow(row)
        print("Results saved to CSV file:", output_csv)


    def compare_results(self):
        """
        COMPARISON PART:
        Compare the extracted results with the groundtruth at a per-field level.
        For each document and for each field of interest, compute a field-specific metric.
        
        For textual fields ("Title" and "Author"), compute a "Textual Score" defined as the average of 
        WER, CER, and Levenshtein distance.
        For non-textual fields ("Publication Year" and "Groundwater level mentioned"), compute more standard metrics:
        precision, recall, and F1 score.
        
        The detailed results (one row per field per document) are saved to a CSV file.
        """
        # Load groundtruth CSV (assumes the CSV has a 'Filename' field)
        gt_csv = self.config.get('groundtruth_csv', 'data/groundtruth.csv')
        groundtruth_data = {}
        with open(gt_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                groundtruth_data[row["Filename"]] = row

        # Load results CSV
        results_csv = self.config.get('output_csv', 'data/extracted_data.csv')
        results = []
        with open(results_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                results.append(row)

        # Define the fields to compare
        fields = ["Title", "Author", "Publication Year", "Groundwater level mentioned"]
        detailed_results = []

        # For each document, compute metrics for each field individually
        for res in results:
            filename = res["Filename"]
            if filename in groundtruth_data:
                gt = groundtruth_data[filename]
                for field in fields:
                    pred_value = res.get(field, "").strip()
                    gt_value = gt.get(field, "").strip()
                    
                    if field in ["Title", "Author"]:
                        # For textual fields, compute WER, CER, and Levenshtein, then average them
                        wer = compute_wer(gt_value, pred_value)
                        cer = compute_cer(gt_value, pred_value)
                        levenshtein = compute_levenshtein_distance(gt_value, pred_value)
                        # We consider a match if the Levenshtein distance is less than 10% of the length of the groundtruth
                        is_found = 1 if levenshtein <= 0.1 else 0
                        detailed_results.append({
                            "Filename": filename,
                            "Field": field,
                            "WER": wer,
                            "CER": cer,
                            "Levenshtein": levenshtein,
                            "Precision": "",   # Not applicable for textual fields
                            "Recall": "",
                            "F1": "",
                            "Found": is_found,
                        })
                    else:
                        # For non-textual fields, compute precision, recall, and F1
                        precision = compute_precision({field: gt_value}, {field: pred_value})
                        recall = compute_recall({field: gt_value}, {field: pred_value})
                        f1 = compute_f1({field: gt_value}, {field: pred_value})
                        is_found = 1 if f1 == 1 else 0
                        detailed_results.append({
                            "Filename": filename,
                            "Field": field,
                            "WER": "", # Not applicable for non-textual fields
                            "CER": "", # Not applicable for non-textual fields
                            "Levenshtein": "", # Not applicable for non-textual fields
                            "Precision": precision,
                            "Recall": recall,
                            "F1": f1,
                            "Found": is_found,
                        })
            else:
                print(f"Warning: No groundtruth found for {filename}")

        # Dynamically create a folder for the detailed comparison results (based on the OCR and LLM models)
        ocr_model = self.config.get('ocr', {}).get('type', 'tesseract').lower()
        llm_model = self.config.get('llm', {}).get('model', 'qwen2.5:7b')
        folder_name = f"results_{ocr_model}_{llm_model}"
        os.makedirs(f"data/results/{folder_name}", exist_ok=True) # Create the folder if it doesn't exist
        file_name = f"data/results/{folder_name}/detailed_results.csv"

        # Write the detailed per-field metrics to a CSV file
        detailed_csv = self.config.get('detailed_comparison_csv', file_name)
        with open(detailed_csv, 'w', newline='', encoding='utf-8') as csvfile:
            # We'll include all potential columns
            fieldnames = ["Filename", "Field", "Textual Score", "WER", "CER", "Levenshtein", "Precision", "Recall", "F1", "Found"]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for row in detailed_results:
                writer.writerow(row)
        print("Detailed per-field comparison results saved to CSV file:", detailed_csv)
        
        # Plot the detailed results
        plot_detailed_results(detailed_csv, output_dir=f"data/results/{folder_name}")
        

    def run(self):
        """
        Main pipeline method.
        First, run the OCR conversion (PDF → Markdown), then perform information extraction.
        """
        self.prepare_data()
        self.run_ocr()
        self.run_search()
        self.compare_results()

if __name__ == "__main__":
    pipeline = Pipeline(config_path="script_config.yaml")
    pipeline.run()